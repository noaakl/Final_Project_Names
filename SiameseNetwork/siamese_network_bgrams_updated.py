# -*- coding: utf-8 -*-
"""Siamese_Network_Bgrams_Updated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17AJAjqKAb5epEYPgNPkkbmwPeBFz4SER

**Imports**
"""

# from google.colab import drive
# drive.mount('/content/drive')

# !pip install turicreate

"""[A Complete Guide to PyTorch for Data Scientists](https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html)"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import torchvision
import torchvision.datasets as dset
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
import torchvision.utils
import numpy as np
import random
from PIL import Image
import torch
from torch.autograd import Variable
import PIL.ImageOps
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn import preprocessing
import pandas as pd
from torchtext.data.utils import get_tokenizer
import csv
import sklearn
import tensorflow as tf
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import turicreate as tc
import os
from tqdm import tqdm
import re
import time
import itertools
from csv import writer
import string

# import chars2vec
# split reference https://towardsdatascience.com/from-words-to-vectors-e24f0977193e

# concat the split files from competitor results
concat_files = False
if concat_files:
    path = "./Siamese_Datasets/Competitor_Results/"
    data_type = "Metaphone"
    df0 = pd.read_csv(path + data_type + "/top_ten_suggestions_for_gt_by_" + data_type + "_0_with_gt.csv")
    df1 = pd.read_csv(path + data_type + "/top_ten_suggestions_for_gt_by_" + data_type + "_1_with_gt.csv")
    df2 = pd.read_csv(path + data_type + "/top_ten_suggestions_for_gt_by_" + data_type + "_2_with_gt.csv")
    df3 = pd.read_csv(path + data_type + "/top_ten_suggestions_for_gt_by_" + data_type + "_3_with_gt.csv")
    df4 = pd.read_csv(path + data_type + "/top_ten_suggestions_for_gt_by_" + data_type + "_4_with_gt.csv")
    dfs = [df0, df1, df2, df3, df4]
    united_df = pd.concat(dfs)
    united_df = united_df.sort_values(by='Original')
    united_df.to_csv(path + data_type + "/" + data_type + "_names.csv")

# use gpu
dev = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")


def main_activation():
    # paths parameters
    # short_file_path = 'drive/MyDrive/Siamese Network Names/Datasets/'
    # long_file_path = 'drive/MyDrive/Siamese Network Names/Datasets/Phonetic_Encoding_Suggestions/'
    short_file_path = './Datasets/'
    long_file_path = './Datasets/Phonetic_Encoding_Suggestions/'

    # Parameter combinations for the network train ('configs')
    train_batch_size = [64]  # [52, 58, 64, 70, 76, 82]
    version2_dataset = [False]  # [True,False] # true -> 2 versions , false -> new datasets only
    epochs = [10]  # [50,60,70,80,90,100,120]
    vector_split_type = ['word_ngrams']  # ,'name2vec']#['Sparse','Dense']
    name_letter_split = [2]  # [1,2,3] fix for not 2
    result_decision_distance = [3, 4, 5, 6, 7, 8]
    datasets = ["knn_suggestions_according_sound_pandas_imp_sorted_by_ed.csv"]
    config = [epochs, vector_split_type, name_letter_split, result_decision_distance,
              datasets, train_batch_size, version2_dataset]
    for config_tuple in list(itertools.product(*config)):
        # activate all functions
        # dev = activate_GPU()
        # init config_tuple values
        epochs = config_tuple[0]
        vector_split_type = config_tuple[1]
        name_letter_split = config_tuple[2]
        result_decision_distance = config_tuple[3]
        datasets = config_tuple[4]
        train_batch_size = config_tuple[5]
        version2_dataset = config_tuple[6]

        filtered_df = filter_names(short_file_path, datasets)  # filter names and save df to file
        # first phase train
        methods_df = first_phase_negative_examples(long_file_path, version2_dataset)
        train_df = create_train_df(methods_df, filtered_df)
        train_data, test_data = train_test_split(train_df)
        train_data = add_random_negatives(train_df, train_data)  # add random easy negatives
        print("done1")
        # second phase train
        unique_df = second_phase_negative_examples(short_file_path, datasets, filtered_df)
        trios = add_random_negatives_phase_two(unique_df)
        train_data, test_data = train_test_split_phase_two(train_data, test_data, trios)
        print("done2")

        train_data_vectors = create_data_vectors(train_data, vector_split_type, name_letter_split)
        train_dataloader = create_siamese_dataset(train_data_vectors, train_batch_size)
        net = train_network(train_dataloader, epochs, embedded_dim)
        print("done_train")
        df_test = prepare_test_data(test_data)
        test_data_vectors = create_data_vectors(test_data, vector_split_type, name_letter_split)
        results = calculate_test_distances(test_data_vectors, train_batch_size, net, df_test)
        print("done_test")
        df_test, accuracy, precision, recall, majority_same = calculate_accuracy(df_test, results,
                                                                                 result_decision_distance)
        show_false_data_results(df_test)
        save_model_and_results(df_test, net, config_tuple)
        add_config_to_csv(config_tuple, accuracy, precision, recall, majority_same)


def filter_rows_by_values(df, col, values):
    return df[~df[col].isin(values)]


def filter_names(short_file_path, dataset):
    """
      The functions filter the names to remove and returns the new df
      The function saves the filtered df as a csv
    """
    vals_toRemove = ['Schleswigholsteinsonderburgglcksburg', 'Mariemargueriteyvetterollande',
                     'Schleswigholsteinsonderburg', 'Mariemadeleineconstance',
                     'Mariemargueriteanglique', 'Josephphilippenapoleon', 'Mariefranoiseangelique',
                     'Mariemadeleineanglique', 'Mariemargueritepascale', 'Jeanbaptistebarthlemy',
                     'Mariemadeleinemalvina', 'Mariemargueritejeanne', 'Charlesambroiseemery',
                     'Jeanbaptistevalentin', 'Jeanbaptistefrancois', 'Jeanbaptistenapoleon',
                     'Josephfrancoisarthur', 'Josephphilippehorace', 'Mariecatherinejoseph',
                     'Mariemargueriteagnes', 'Mariemargueriteccile', 'Jeanbaptisteadolphe',
                     'Jeanbaptisteauguste', 'Jeanbaptistemathieu', 'Jeanbaptisteantoine',
                     'Jeanbaptistefranois', 'Jeanbaptistebastien', 'Jeanbaptistegervais', 'Xxxxxxxxx']
    # The ground truth is pair of Original and Candidate with distance 0.
    # Pairs with distance 0 are pairs that sounded the same
    pd_reader = pd.read_csv(short_file_path + dataset)  # "knn_suggestions_according_sound_pandas_imp_sorted_by_ed.csv")
    filtered_df = pd_reader[pd_reader["Distance"] == 0]
    filtered_df = filter_rows_by_values(filtered_df, "Candidate", vals_toRemove)
    filtered_df = filter_rows_by_values(filtered_df, "Original", vals_toRemove)
    # save 'our' df as ground truth of spokenName2Vec
    filtered_df.to_csv("spokenName2Vec_ground_truth.csv")
    return filtered_df


# !!!!!!!!!!!!!!!!!!!!!!!!!!!!NEEDED?
# TODO: CHECK IF NEEDED
# Randomly choosing a negative sample (easy negative examples)
def negative(originals, index):
    neg = "None"
    # Each name might be in the dataset 10 times in a row (at most)
    if index <= 9:
        neg = originals[random.randint(10, len(originals) - 1)]
    elif index >= len(originals) - 10:
        neg = originals[random.randint(0, len(originals) - 10)]
    else:
        above = originals[random.randint(0, index - 10)]
        below = originals[random.randint(index + 10, len(originals) - 1)]
        neg = random.choice([above, below])
    return neg


"""
First method to create the train data
"""

'''
Phase to add Negative Examples from competitors
'''


def first_phase_negative_examples(long_file_path, version2):
    """
    The method reads the competitor csv files and creates a df of the names the
    algorithm did not agree with 'Behind The Name' as negative examples
    """
    # filtered_df -> positive (by us, sound2vec) , methods_df -> negative (synonym 0 by behind the name)
    # Original -> Name appeared on our df and on competitors' negatives
    # Candidate_x -> competitors' incorrect suggestion for name in 'Original' (synonym 0)
    # Candidate_y -> our correct suggestion for name in 'Original'

    if version2:  # the 2 datasets
        # Suggestion of the competitors
        dm_df = pd.read_csv(long_file_path + 'top_ten_suggestions_for_gt_by_Double_Metaphone_with_gt.csv')
        mrc_df = pd.read_csv(long_file_path + 'top_ten_suggestions_for_gt_by_Matching_Rating_Codex_with_gt.csv')
        metaphone_df = pd.read_csv(long_file_path + 'top_ten_suggestions_for_gt_by_Metaphone_with_gt.csv')
        nysiis_df = pd.read_csv(long_file_path + 'top_ten_suggestions_for_gt_by_Nysiis_with_gt.csv')
        soundex_df = pd.read_csv(long_file_path + 'top_ten_suggestions_for_gt_by_Soundex_with_gt.csv')
        # Suggestion of the competitors version 2
        dm_v2_df = pd.read_csv(long_file_path + 'v2_top_ten_suggestions_for_gt_by_Double_Metaphone_with_gt.csv')
        mrc_v2_df = pd.read_csv(long_file_path + 'v2_top_ten_suggestions_for_gt_by_Matching_Rating_Codex_with_gt.csv')
        metaphone_v2_df = pd.read_csv(long_file_path + 'v2_top_ten_suggestions_for_gt_by_Metaphone_with_gt.csv')
        nysiis_v2_df = pd.read_csv(long_file_path + 'v2_top_ten_suggestions_for_gt_by_Nysiis_with_gt.csv')
        soundex_v2_df = pd.read_csv(long_file_path + 'v2_top_ten_suggestions_for_gt_by_Soundex_with_gt.csv')
        # Adding the name of the method as a column
        dm_v2_df['Method'] = 'Double Metaphone'
        mrc_v2_df['Method'] = 'Matching Rating Codex'
        metaphone_v2_df['Method'] = 'Metaphone'
        nysiis_v2_df['Method'] = 'Nysiis'
        soundex_v2_df['Method'] = 'Soundex'
    else:  # use the new competitor dataset
        # Suggestion of the competitors
        # path = "/content/drive/MyDrive/Siamese_Datasets/Competitor_Results/"
        path = './Siamese_Datasets/Competitor_Results/'
        dm_df = pd.read_csv(path + 'Double_Metaphone/Double_Metaphone_names.csv')
        mrc_df = pd.read_csv(path + 'Matching_Rating_Codex/Matching_Rating_Codex_names.csv')
        metaphone_df = pd.read_csv(path + 'Metaphone/Metaphone_names.csv')
        nysiis_df = pd.read_csv(path + 'Nysiis/Nysiis_names.csv')
        soundex_df = pd.read_csv(path + 'Soundex/Soundex_names.csv')

    # Adding the name of the method as a column
    dm_df['Method'] = 'Double Metaphone'
    mrc_df['Method'] = 'Matching Rating Codex'
    metaphone_df['Method'] = 'Metaphone'
    nysiis_df['Method'] = 'Nysiis'
    soundex_df['Method'] = 'Soundex'
    if not version2:
        # Concat the datasets
        dfs = [dm_df, mrc_df, metaphone_df, nysiis_df, soundex_df]
    else:
        # Concat the datasets
        dfs = [dm_df, mrc_df, metaphone_df, nysiis_df, soundex_df, dm_v2_df, mrc_v2_df,
               metaphone_v2_df, nysiis_v2_df, soundex_v2_df]
    methods_df = pd.concat(dfs)
    # Find the ones the algorithms got wrong
    # FOR HARD NEGATIVE EXAMPLES OF COMPETITORS
    methods_df = methods_df[methods_df['Is_Original_Synonym'] == 0]
    return methods_df


"""We take the mistakes of the competitors and use the as negative samples"""


def create_train_df(methods_df, filtered_df):
    """
    The method merges the positive and negative dfs and creates the train df with the candidates
    """
    originals_df = pd.merge(methods_df, filtered_df, how="inner", left_on='Original', right_on='Original')
    train_df = originals_df[['Original', 'Candidate_y', 'Candidate_x']]
    train_df = train_df.rename(columns={'Candidate_y': 'Positive',
                                        'Candidate_x': 'Negative'})
    return train_df


# TODO: CHECK IF NEEDED
def helper2():  # #!!!!!!!!!!!!!!!!!!!!!!!!!!!!NEEDED? CHECK
    # CHECK IF NEEDED --> ADDING MIXED NEGATIVE EXAMPLES.. (not complicated negatives)

    # Samples From ground truth we created from turicreate dataset
    ground_truth_df = pd.read_csv("./spokenName2Vec_ground_truth.csv")
    # For each name we mark the letter it starts with
    ground_truth_df["Start"] = ground_truth_df["Original"].apply(
        lambda x: x[0])
    originals = ground_truth_df['Original']
    negatives = []
    # Creating random negatives
    for i in tqdm(range(len(originals))):
        negatives += [negative(originals, i)]

    # Updating the ground truth
    ground_truth_df['Negative'] = negatives
    # ground_truth_df
    ground_truth_df.to_csv('spokenName2Vec_ground_truth.csv', index=False)
    # df = ground_truth_df[ground_truth_df["Start"] == "A"]


# TODO: CHECK IF NEEDED
def helper():  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!NEEDED? CHECK
    helper2()
    # CHECK IF NEEDED --> ADDING MIXED NEGATIVE EXAMPLES.. (not complicated negatives)
    with open('./spokenName2Vec_ground_truth.csv', newline='') as f:
        reader = csv.reader(f)
        data = list(reader)
    triplets = []
    for row in tqdm(data):
        triplets = triplets + [[row[2], row[3], row[8]]]
    triplets = triplets[1:]
    random.shuffle(triplets)
    return triplets


def train_test_split(train_df):
    # Split to train data and test data
    ### ADD EXTRA RANDOM NEGATIVES (adding the triplets..)
    triplets = helper()
    originals = train_df['Original']
    pos = list(train_df['Positive'])
    for row in tqdm(train_df.itertuples()):
        triplets = triplets + [[row[1], row[2], row[3]]]
    triplets = triplets[1:]

    l = int(0.66 * len(triplets))
    train_data = triplets[:l]
    test_data = triplets[l:]
    # test_data
    ### WITHOUT THE EXTRA RANDOM NEGATIVES USE THIS BELOW
    # l = int(0.66 * len(train_df))
    # train_data = train_df[:l]
    # test_data = train_df[l:]
    # test_data
    return train_data, test_data


# TODO: CHECK IF NEEDED - yes..?
def add_random_negatives(train_df, train_data):  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!NEEDED?
    ################check if needed (used to add more random negatives) -> random negative for every unique positive
    # Samples from competitors
    """
    The method adds random 'easy' negative examples to the train data
    """
    train_array = train_df['Positive'].unique()
    competitors = []
    for pos in tqdm(train_array):
        pos_df = train_df[train_df['Positive'] == pos]
        org = random.choice(pos_df['Original'].to_list())
        postv = random.choice(pos_df['Positive'].to_list())
        negtv = random.choice(pos_df['Negative'].to_list())
        competitors += [[org, postv, negtv]]
    train_data = train_data + competitors
    return train_data


"""**Data from competitors**

Second method to create the train-data.
All Negatives in this method are mistakes of competitors
"""

'''
Phase to add Negative Examples from methods of name representation as vectors (can switch with larger file later)
'''


def second_phase_negative_examples(short_file_path, dataset, filtered_df):
    '''
  The method reads the vector representation csv files and creates a df of the names the 
  algorithms' received distance was large as negative examples
  '''
    # The three methods to represent the names as vectors
    turicreate_df = pd.read_csv(short_file_path + dataset)
    turicreate_df['Method'] = 'turicreate'
    wav2vec_df = pd.read_csv(short_file_path + "knn_suggestions_according_sound_pandas_imp_wav2vec.csv")
    wav2vec_df['Method'] = 'wav2vec'
    pyAudioAnalysis_df = pd.read_csv(short_file_path + "knn_suggestions_according_sound_pandas_imp_pyAudioAnalysis.csv")
    pyAudioAnalysis_df['Method'] = 'pyAudioAnalysis'
    # add negative examples from vector representation methods
    # turicrete mistakes have larger distance from other methods
    turicreate_df = turicreate_df[turicreate_df['Distance'] >= 4]  # add distance to parameters?
    dfs = [turicreate_df, wav2vec_df, pyAudioAnalysis_df]
    methods_df = pd.concat(dfs)
    methods_df = methods_df[methods_df['Distance'] >= 1]  # add distance to parameters?
    methods_df = methods_df.rename(columns={'Candidate': 'Negative'})
    filtered_df = filtered_df.rename(columns={'Candidate': 'Positive',
                                              'Distance': 'Distance_Spoken_Name',
                                              'Edit_Distance': 'Edit_Distance_Spoken_Name'})
    # merge the vector representation df with the filtered df
    sn_gt_df = pd.merge(methods_df, filtered_df, how="inner", left_on='Original', right_on='Original')
    # trim the df
    triplets_df = sn_gt_df[['Original', 'Positive', 'Negative']]
    # Drop duplicate (original + positive) pairs
    unique_df = triplets_df.drop_duplicates(subset=['Original', 'Positive'])
    return unique_df


# !!!!!!!!!!!!!!!! NEEDED? CHECK
# CHECK IF NEEDED AFTER DUPLICATE REMOVE FUNCTION
# # find unique pairs(original, positive)
# originals = triplets_df['Original'].tolist()
# positives = triplets_df['Positive'].tolist()
# negatives = triplets_df['Negative'].tolist()

# pairs = []
# for i, org in enumerate(originals):
#   pairs += [org + positives[i]]

# pairs_df = pd.DataFrame({'Pairs': pairs, 'originals': originals, 
#                          'Positive': positives, 'Negative': negatives})

def add_random_negatives_phase_two(unique_df):
    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!NEEDED?
    # create triplets of data ###################check if needed (easy negative examples)
    originals = unique_df['Original'].tolist()
    positives = unique_df['Positive'].tolist()
    negatives = unique_df['Negative'].tolist()
    trios = []
    for i, org in enumerate(originals):
        trios += [[org, positives[i], negatives[i]]]
        random_negative = negative(originals, i)
        trios += [[originals[i], positives[i], random_negative]]

    random.shuffle(trios)

    print(trios[0])
    print(len(trios))
    return trios


def train_test_split_phase_two(train_data, test_data, trios):
    # l = int(0.66 * len(unique_df))
    # train_data = unique_df[:l]
    # test_data = unique_df[l:]
    # test_data
    ######################### DELETE IF NOT NEEDED TO ADD RANDOM NEGATIVES (use above)
    l = int(0.66 * len(trios))
    train_data += trios[:l]
    test_data += trios[l:]
    return train_data, test_data


"""**Creating Vectors**

In this notebook we use an embedding method inspired by FastText sub-word generation. For a word, we generate charecter b-grams in it. 


*   We take a name and add angular brackets to denote the beginning and end of a word. E.g **Marc** --> **\<marc>**
*   Then, we generate charecter b-grams. For exaple: **\<marc>** --> **[<m, ma, ar, rc, c>]**
*   Then, we take each pair of letter as a number in base 28 and convert them to base 10. ('<' = 26, '>' = 27)

We have two ways to represent the vectors:
1. Dense - A list of zeros and ones. We put 1 in the indexes that represent the b-grams. For exaple, the list **[<a, aa, a>]** will have ones in 728, 0 and 27.
2. Sparse- Pytorch has a special object that helps represent big vectors with a lot of zeros in more efficient way. You can read about sparse [here](https://pytorch.org/docs/stable/sparse.html).





"""


# Convert names to sparse
def word2sparse(name, name_letter_split):
    name = name.lower()
    name = [26] + [ord(c) - ord('a') for c in name] + [27]
    name = [name[i:i + name_letter_split] for i in range(len(name) - 1)]
    nameidx = torch.tensor([[0, pair[0] * 28 + pair[1]] for pair in name])
    values = torch.ones(len(name))
    s = torch.sparse_coo_tensor(nameidx.t(), values, (1, 28 ** 2))
    return s, 28 ** 2


# Convert names to dense
def word2dense(name, name_letter_split):
    name = name.lower()
    name = [26] + [ord(c) - ord('a') for c in name] + [27]
    name = [name[i:i + name_letter_split] for i in range(len(name) - 1)]
    nameidx = [pair[0] * 28 + pair[1] for pair in name]
    dense = torch.zeros(28 ** 2)
    for idx in nameidx:
        dense[idx] = 1
    return dense, 28 ** 2


def create_grams_dict(number_of_grams):
    """
    The function creates a gram dictionary using the number_of_grams input given in order to creat a representation
    for the letter combinations
    """
    alphabet = list(string.ascii_lowercase)
    if number_of_grams == 1:
        alph_dict = dict((alphabet[indx], indx) for indx in range(len(alphabet)))
    else:
        letter_combinations = [''.join(tup) for tup in list(itertools.product(alphabet, repeat=number_of_grams))]
        alph_dict = dict((letter_combinations[idx], idx) for idx in range(len(letter_combinations)))
    return alph_dict, len(alph_dict)


def word_ngrams(name, number_of_grams):
    """
    The function splits the name into the given amount of chars in 'number_of_grams' and return a vector representation using
    'create_grams_dict'
    """
    alph_dict, vector_size = create_grams_dict(number_of_grams)
    name = name.lower()
    # first num is grams second num is grams-1 for split
    name_split = [name[i:i+number_of_grams] for i in range(len(name)-(number_of_grams-1))]
    name_vec = torch.tensor([[0,alph_dict[chars]] for chars in name_split])
    values = torch.zeros(len(name_vec))
    s = torch.sparse_coo_tensor(name_vec.t(), values, (1,vector_size))
    return s, vector_size


# convert names to vec representation by using char2vec
# def name2vec(name,name_letter_split):
#   name = name.lower()
#   c2v_model = chars2vec.load_model('eng_50')
#   word_embeddings = c2v_model.vectorize_words([name])
#   return word_embeddings.__getitem__(0)

# convert names to vec representation by using addition
def name2vec_addition(name, name_letter_split):
    name = name.lower()
    name = [26] + [ord(c) - ord('a') for c in name] + [27]
    name = [name[i:i + 2] for i in range(len(name) - 1)]
    sum_pair = [0, 0]
    for pair in name:
        sum_pair[0] += pair[0]
        sum_pair[1] += pair[1]
    return sum_pair


def create_data_vectors(data, split_method, name_letter_split):
    data_vectors = []
    dim = 28 ** 2

    if split_method == 'Sparse':
        for triplet in data:
            data_vectors += [[word2sparse(triplet[0], name_letter_split)[0].to(dev),
                              word2sparse(triplet[1], name_letter_split)[0].to(dev),
                              word2sparse(triplet[2], name_letter_split)[0].to(dev)]]
    # elif split_method == 'name2vec':
    #     for triplet in data:
    #         data_vectors += [[name2vec(triplet[0], name_letter_split).to(dev),
    #                           name2vec(triplet[1], name_letter_split).to(dev),
    #                           name2vec(triplet[2], name_letter_split).to(dev)]]
    elif split_method == 'word_ngrams':
        for triplet in data:
            org = word_ngrams(triplet[0], name_letter_split)[0]
            dim = word_ngrams(triplet[0], name_letter_split)[1]
            data_vectors += [[org,
                              word_ngrams(triplet[1], name_letter_split)[0],
                              word_ngrams(triplet[2], name_letter_split)[0]]]
    else:
        for triplet in data:
            data_vectors += [[word2dense(triplet[0], name_letter_split)[0].to(dev),
                              word2dense(triplet[1], name_letter_split)[0].to(dev),
                              word2dense(triplet[2], name_letter_split)[0].to(dev)]]
    return data_vectors , dim


"""## **Siamese Network**
**With PyTorch**

building and training the network
"""


class SiameseNetworkDataset(Dataset):

    def __init__(self, data):
        self.data = data

    def __getitem__(self, index):
        anch = self.data[index][0]
        pos = self.data[index][1]
        neg = self.data[index][2]

        return anch, pos, neg

    def __len__(self):
        return len(self.data)


def create_siamese_dataset(train_data_vectors, train_batch_size):
    """
    The method creates the siamese dataset and wraps with dataloader to loop through the set rows
    """
    siamese_dataset = SiameseNetworkDataset(train_data_vectors)
    # DataLoaders help arrange the data
    train_dataloader = DataLoader(siamese_dataset,
                                  shuffle=True,
                                  num_workers=0,
                                  batch_size=train_batch_size)
    # batch_size=Config.train_batch_size)
    return train_dataloader


class SiameseNetwork(nn.Module):
    def __init__(self, embed_dim):
        super(SiameseNetwork, self).__init__()
        self._lw1 = torch.nn.parameter.Parameter(torch.randn(784, 512))
        self._l1 = nn.Linear(embed_dim, 512)
        self._relu = nn.ReLU(inplace=True)
        self._l2 = nn.Linear(512, 128)
        self._l3 = nn.Linear(128, 10)

    def forward_once(self, x):
        # if you use dense vectors use the row below instead of x.bmm and lw1
        # x = self._l1(x)
        b = x.shape[0]
        x = x.bmm(self._lw1.repeat(b, 1, 1))
        x = self._relu(x)
        x = self._l2(x)
        x = self._relu(x)
        x = self._l3(x)
        return x

    def forward(self, input1, input2, input3):
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)
        output3 = self.forward_once(input3)
        return output1.to(dev), output2.to(dev), output3.to(dev)


"""You can read about *TripletMarginLoss* [here](https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html)."""


def show_plot(iteration, loss):
    plt.plot(iteration, loss)
    plt.show()


def train_network(train_dataloader, train_number_epochs, embedded_dim):
    """
    Train the network and calculate loss
    """
    net = SiameseNetwork(embedded_dim).to(dev) # embedded_dim = 28 ** 2
    # if you get "expected double" eror while trying to train the model, add:
    # net = net.double()
    criterion = nn.TripletMarginLoss(margin=0.5)
    optimizer = optim.Adam(net.parameters(), lr=0.0005)
    counter = []
    loss_history = []
    iteration_number = 0

    net.train()
    last_epochs_avg = 0
    for epoch in range(0, train_number_epochs):
        T = time.time()
        for i, data in enumerate(train_dataloader, 0):
            anchors, positives, negatives = data
            negatives = torch.as_tensor(negatives).to(dev)
            anchors, positives = torch.as_tensor(anchors).to(dev), torch.as_tensor(positives).to(dev)
            optimizer.zero_grad()
            output1, output2, output3 = net(anchors, positives, negatives)
            loss_contrastive = criterion(output1, output2, output3)
            loss_contrastive.backward()
            optimizer.step()

            # print starting value
            if epoch == 0 and i == 0:
                print("Start\n Current loss {}\n".format(loss_contrastive.item()))
                iteration_number += 10
                counter.append(iteration_number)
                loss_history.append(loss_contrastive.item())

        print("Epoch number {}\n Current loss {}\n".format(epoch, loss_contrastive.item()))
        ### added code for avg epoch loss
        if epoch + 3 >= train_number_epochs:
            last_epochs_avg += loss_contrastive.item()
        ##################3
        iteration_number += 10
        counter.append(iteration_number)
        loss_history.append(loss_contrastive.item())
        T = time.time() - T
        print("Time: {}\n".format(T))
    #####
    last_epochs_avg = last_epochs_avg / 3
    print("last 3 epochs average: ", last_epochs_avg)
    #####
    net.eval()
    show_plot(counter, loss_history)
    return net


def prepare_test_data(test_data):  # Creating DataFrame from the test-data
    anch = []
    candidate = []
    labels = []
    # create 2 rows out of each tuple row in the data -->
    # anchor, candidate_pos, positive | anchor, candidate_neg, negative
    for row in test_data:  # .itertuples():
        # row1
        anch += [row[0]]
        candidate += [row[1]]
        labels += ['Positive']
        # row2
        anch += [row[0]]
        labels += ['Negative']
        candidate += [row[2]]
    data = {'Original': anch, 'Candidate': candidate, 'Type': labels}
    df_test = pd.DataFrame(data)
    return df_test


def calculate_test_distances(test_data_vectors, train_batch_size, net, df_test):
    siamese_dataset = SiameseNetworkDataset(test_data_vectors)

    results = []
    test_dataloader = DataLoader(siamese_dataset, num_workers=0, batch_size=train_batch_size, shuffle=False)
    dataiter = iter(test_dataloader)
    dist = []
    for i in range(len(test_dataloader)):
        x0, x1, label2 = next(dataiter)
        x0, x1, label2 = torch.as_tensor(x0).to(dev), torch.as_tensor(x1).to(dev), torch.as_tensor(label2).to(dev)
        output1, output2, output3 = net(x0, x1, label2)

        # for each trio we calculate:
        # 1. The distance of the positive sample from the original sample
        # 2. The distance of the negative sample from the original sample
        for j in range(len(output1)):
            euclidean_distance1 = F.pairwise_distance(output1[j], output2[j])
            euclidean_distance2 = F.pairwise_distance(output1[j], output3[j])
            results += [[euclidean_distance1.item(), euclidean_distance2.item()]]
            # for idx in range(len(euclidean_distance1)):
            dist += [euclidean_distance1.item()]
            dist += [euclidean_distance2.item()]
    # dist
    df_test['Distance'] = dist
    # df_test
    # dist
    # print(results[0:64])
    return results


def calculate_accuracy(df_test, results,
                       result_decision_distance):  # We use the distances to predict whether two names sound the same (experiment with the distance.. maybe mean)
    actual = []  # the real type of the pair
    net_result = []  # the result from the network
    for r in results:
        if r[0] <= result_decision_distance:
            actual += ['same']
            net_result += ['same']
        else:
            actual += ['same']
            net_result += ['different']
        if r[1] <= result_decision_distance:
            net_result += ['same']
            actual += ['different']
        else:
            net_result += ['different']
            actual += ['different']
    df_test['Actual'] = actual
    df_test['Network_Prediction_Result'] = net_result
    accuracy = accuracy_score(actual, net_result)
    print("Accuracy ", accuracy)
    ##PRECISION
    macro = precision_score(net_result, actual, average='macro')
    micro = precision_score(net_result, actual, average='micro')
    weighted = precision_score(net_result, actual, average='weighted')
    print("Precision: macro = " + str(macro), "micro = " + str(micro), "weighted = " + str(weighted))
    precision = weighted
    ##RECALL
    macro = recall_score(net_result, actual, average='macro')
    micro = recall_score(net_result, actual, average='micro')
    weighted = recall_score(net_result, actual, average='weighted')
    print("Recall: macro = " + str(macro), "micro = " + str(micro), "weighted = " + str(weighted))
    recall = weighted
    # check the accuracy - what is the majority and if they are always chosen
    majority_same = df_test['Actual'].value_counts()['same'] / df_test['Actual'].size
    return df_test, accuracy, precision, recall, majority_same


def show_false_data_results(df_test):
    """
    The method prints useful data results of false predictions by the network
    """
    print('FalseDataResults:')
    # print(df_test)
    print(df_test[df_test["Actual"] != df_test["Network_Prediction_Result"]])
    print("Actual: same, Prediction: different ",
          df_test[((df_test["Actual"] == 'same') & (df_test["Network_Prediction_Result"] == 'different'))][
              "Distance"].mean())
    print("Actual: different, Prediction: same ",
          df_test[((df_test["Actual"] == 'different') & (df_test["Network_Prediction_Result"] == 'same'))][
              "Distance"].mean())
    # print(df_test[((df_test["Actual"] == 'same') & (df_test["Network_Prediction_Result"]=='different'))])


"""xfff"""


def save_model_and_results(df_test, net, config_tuple):  # net? to save
    # Saving the results to csv allows us to look on all the samples
    df_test.to_csv('./TestDataResults/Test_Data_and_Results_' + str(config_tuple) + '.csv')
    # Saving the model
    # PATH = 'model.pkl'
    # torch.save(net.state_dict(), PATH)


# append the tuples to the csv file
def add_config_to_csv(config_tuple, accuracy, precision, recall, majority_same):
    tup = []
    for config in config_tuple:
        tup += [config]
    tup += [accuracy] + [precision] + [recall] + [majority_same] + [1 - majority_same]  # diff
    print(tup)
    # fields = ['Epochs','Vector_Split_Type','Name_Letter_Split','Result_Decision_Distance',
    #           'Datasets','Train_Batch_Size']
    # Open our existing CSV file in append mode
    # Create a file object for this file
    with open('./Siamese_Parameter_Runs.csv', 'a') as f_object:
        # Pass this file object to csv.writer()
        # and get a writer object
        writer_object = writer(f_object)
        # Pass the list as an argument into
        # the writerow()
        writer_object.writerow(tup)
        # Close the file object
        f_object.close()


main_activation()

"""**Precision**

**Recall**

# 
# ###**Small Test**
# Using small dataset in order to save time for a simple test
# """
#
# test_df_first = pd.read_csv("all_distinct_names_length_higher_than_2_characters.csv") # a list of first names
# test_data = test_df_first['Name']
# tmp = []
# for i in range(0, len(test_data), 3):
#   if i + 2 < len(test_data):
#     tmp += [[test_data[i], test_data[i + 1], test_data[i + 2]]]
# test_data = tmp
# names = tmp
#
# vectors_b = []
# for triplet in test_data:
#   vectors_b += [[word2sparse(triplet[0]).to(dev),
#                  word2sparse(triplet[1]).to(dev),
#                  word2sparse(triplet[2]).to(dev)]]
# test_data_vectors = vectors_b
#
# siamese_dataset = SiameseNetworkDataset(test_data_vectors)
# test_dataloader = DataLoader(siamese_dataset, num_workers=0, batch_size=64, shuffle=False)
# dataiter = iter(test_dataloader)
#
# results = []
# dim = 10
#
# for x0,x1, x2 in dataiter:
#     output1,output2, output3 = net(torch.as_tensor(x0).to(dev),torch.as_tensor(x1).to(dev), torch.as_tensor(x2).to(dev))
#     results += [[output1, output2, output3]]
#
# count = 0
# names_list =[]
# outputs_list = []
# for r in results:
#   for j in range(len(r[0])):
#     names_list += [names[count][0], names[count][1], names[count][2]]
#     outputs_list += [r[0][j], r[1][j],  r[2][j]]
#     count += 1
# emmbedings = []
# for i in range(dim):
#   emmbedings += [[]]
# print(len(emmbedings))
# for output in tqdm(outputs_list):
#   for j in range(dim):
#     emmbedings[j] += [output[0][j].item()]
#
# list_for_dict = [('Name', names_list)]
# for i, e in enumerate(emmbedings):
#   list_for_dict += [('e{}'.format(i), emmbedings[i])]
#
# data = dict(list_for_dict)
# emm_df = pd.DataFrame(data)
# data
#
# """Skip 7 blocks to continue this test
#
# # **Test**
# """
#
# test_df_first = pd.read_csv("all_distinct_First_Name_wikitree.csv") # list of first names and synonyms
# test_data = test_df_first['Child_First_Name']
# tmp = []
# for i in range(0, len(test_data), 3):
#   if i + 2 < len(test_data):
#     tmp += [[test_data[i], test_data[i + 1], test_data[i + 2]]]
# test_data = tmp
# names = tmp
#
# def transform_data(data):
#   transformed_data = []
#   for threesome in test_data:
#     transformed_data += [[word2sparse(threesome[0]).to(dev),
#                           word2sparse(threesome[1]).to(dev),
#                           word2sparse(threesome[2]).to(dev)]]
#   return transformed_data
#
# # DataLoader sometumes has problems with big lists
# data1 = test_data[:50000]
# data2 = test_data[50000:100000]
# data3 = test_data[100000:150000]
# data4 = test_data[150000:200000]
# data5 = test_data[200000:]
# data1 = transform_data(data1)
# data2 = transform_data(data2)
# data3 = transform_data(data3)
# data4 = transform_data(data4)
# data5 = transform_data(data5)
#
# data1_part1 = data1[:25000]
# data2_part1 = data2[:25000]
# data3_part1 = data3[:25000]
# data4_part1 = data4[:25000]
# data5_part1 = data5[:25000]
# data1_part2 = data1[25000:]
# data2_part2 = data2[25000:]
# data3_part2 = data3[25000:]
# data4_part2 = data4[25000:]
# data5_part2 = data5[25000:]
#
# data = [data1_part1, data1_part2, data2_part1, data2_part2, data3_part1,
#         data3_part2, data4_part1, data4_part2, data5_part1, data5_part2]
#
# for subdata in data:
#   siamese_dataset = SiameseNetworkDataset(subdata)
#   test_dataloader = DataLoader(siamese_dataset, num_workers=2, batch_size=64, shuffle=False, collate_fn=collate)
#   for x0,x1,x2 in tqdm(test_dataloader):
#     # x0,x1,x2 = data
#     x2 = x2.cuda()
#     output1,output2, output3 = net(torch.as_tensor(x0).cuda(),torch.as_tensor(x1).cuda(), x2)
#     results += [[output1, output2, output3]]
#
# count = 0
# names_list =[]
# outputs_list = []
# for r in results:
#   for j in range(len(r[0])):
#     try:
#       names_list += [names[count][0], names[count][1], names[count][2]]
#       outputs_list += [r[0][j], r[1][j],  r[2][j]]
#       count += 1
#     except:
#       print("index:", count)
# emmbedings = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]
# print(len(emmbedings))
# for output in tqdm(outputs_list):
#   emmbedings[0] += [output[0].item()]
#   emmbedings[1] += [output[1].item()]
#   emmbedings[2] += [output[2].item()]
#   emmbedings[3] += [output[3].item()]
#   emmbedings[4] += [output[4].item()]
#   emmbedings[5] += [output[5].item()]
#   emmbedings[6] += [output[6].item()]
#   emmbedings[7] += [output[7].item()]
#   emmbedings[8] += [output[8].item()]
#   emmbedings[9] += [output[9].item()]
#   # emmbedings[10] += [output[10].item()]
#   # emmbedings[11] += [output[11].item()]
#   # emmbedings[12] += [output[12].item()]
#   # emmbedings[13] += [output[13].item()]
#   # emmbedings[14] += [output[14].item()]
#   # emmbedings[15] += [output[15].item()]
#   # emmbedings[16] += [output[16].item()]
#   # emmbedings[17] += [output[17].item()]
#   # emmbedings[18] += [output[18].item()]
#   # emmbedings[19] += [output[19].item()]
#   # emmbedings[20] += [output[20].item()]
#   # emmbedings[21] += [output[21].item()]
#   # emmbedings[22] += [output[22].item()]
#   # emmbedings[23] += [output[23].item()]
#   # emmbedings[24] += [output[24].item()]
#   # emmbedings[25] += [output[25].item()]
#
# data = {'Name': names_list, 'e0': emmbedings[0], 'e1': emmbedings[1],
#         'e2': emmbedings[2], 'e3': emmbedings[3], 'e4': emmbedings[4],
#         'e5': emmbedings[5], 'e6': emmbedings[6], 'e7': emmbedings[7],
#         'e8': emmbedings[8], 'e9': emmbedings[9]} # , 'e10': emmbedings[10],
#         # 'e11': emmbedings[11], 'e12': emmbedings[12], 'e13': emmbedings[13],
#         # 'e14': emmbedings[14], 'e15': emmbedings[15], 'e16': emmbedings[16],
#         # 'e17': emmbedings[17], 'e18': emmbedings[18], 'e19': emmbedings[19],
#         # 'e20': emmbedings[20], 'e21': emmbedings[21], 'e22': emmbedings[22],
#         # 'e23': emmbedings[23], 'e24': emmbedings[24], 'e25': emmbedings[25]}
# emm_df = pd.DataFrame(data)
#
# emm_df
#
# feature_names = []
# for i in range(dim):
#     feature_name = f"e{i}"
#     feature_names.append(feature_name)
#
# data = tc.SFrame(emm_df)
# data.shape
#
# """# We use the results from the siamese network and knn in order to find 10 nearest names."""
#
# model = tc.nearest_neighbors.create(data, features=feature_names)
#
# # calculate KNN for all names
# knn = model.query(data, k=11)
# # remove the name itself from its list
# sf = knn[knn['query_label'] != knn['reference_label']]
# sf.materialize()
#
# sf.export_csv('knn_results_with_indexes.csv')
# print("Done!!")
#
# suggestions_df = sf.to_dataframe()
#
# test_df_first = test_df_first.reset_index()
#
# names_dict = {"name": names_list + ["unk"], "index": test_df_first.index}
# # You should check why are the lists don't have the same length
# names_df = pd.DataFrame(names_dict)
#
# names_df
#
# knn_suggestions_df = pd.merge(suggestions_df, names_df, how="inner", left_on='query_label', right_on='index')
#
# knn_suggestions_df = knn_suggestions_df.rename(columns={"index": "Original_index",
#                                                         "name": "Original"})
# knn_suggestions_df
#
# knn_suggestions_df = pd.merge(knn_suggestions_df, names_df, how="inner", left_on='reference_label', right_on='index')
# knn_suggestions_df = knn_suggestions_df.rename(columns={"index": "Candidate_index",
#                                                         "name": "Candidate",
#                                                        "distance": "Distance",
#                                                        "rank": "Rank"})
# knn_suggestions_df = knn_suggestions_df[["Original", "Candidate", "Distance", "Rank"]]
# knn_suggestions_df = knn_suggestions_df.sort_values(by=['Original', 'Rank'], ascending=True)
# knn_suggestions_df
#
# knn_suggestions_df.to_csv("Final_Results.csv")
#
# knn_suggestions_better_df = knn_suggestions_df[knn_suggestions_df['Distance'] == 0]
# knn_suggestions_better_df.to_csv("Final_Results_Zeros.csv")
#
# knn_suggestions_better_df
