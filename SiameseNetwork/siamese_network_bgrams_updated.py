# -*- coding: utf-8 -*-
"""Siamese_Network_Bgrams_Updated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17AJAjqKAb5epEYPgNPkkbmwPeBFz4SER

**Imports**
"""

# from google.colab import drive
# drive.mount('/content/drive')

# !pip install turicreate

"""[A Complete Guide to PyTorch for Data Scientists](https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html)"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import torchvision
import torchvision.datasets as dset
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
import torchvision.utils
import numpy as np
import random
from PIL import Image
import torch
from torch.autograd import Variable
import PIL.ImageOps
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn import preprocessing
import pandas as pd
from torchtext.data.utils import get_tokenizer
import csv
import sklearn
import tensorflow as tf
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import turicreate as tc
import os
from tqdm import tqdm
import re
import time
import itertools
from csv import writer
import pickle

import phase_one_train
import phase_two_train
import name_representation
import siamese_network

# import chars2vec
# split reference https://towardsdatascience.com/from-words-to-vectors-e24f0977193e

# concat the split files from competitor results
concat_files = False
if concat_files:
    path = "./Siamese_Datasets/Competitor_Results/"
    data_type = "Metaphone"
    df0 = pd.read_csv(path + data_type + "/top_ten_suggestions_for_gt_by_" + data_type + "_0_with_gt.csv")
    df1 = pd.read_csv(path + data_type + "/top_ten_suggestions_for_gt_by_" + data_type + "_1_with_gt.csv")
    df2 = pd.read_csv(path + data_type + "/top_ten_suggestions_for_gt_by_" + data_type + "_2_with_gt.csv")
    df3 = pd.read_csv(path + data_type + "/top_ten_suggestions_for_gt_by_" + data_type + "_3_with_gt.csv")
    df4 = pd.read_csv(path + data_type + "/top_ten_suggestions_for_gt_by_" + data_type + "_4_with_gt.csv")
    dfs = [df0, df1, df2, df3, df4]
    united_df = pd.concat(dfs)
    united_df = united_df.sort_values(by='Original')
    united_df.to_csv(path + data_type + "/" + data_type + "_names.csv")

# use gpu
dev = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")


def main_activation():
    start_time = time.time()
    # paths parameters
    # short_file_path = 'drive/MyDrive/Siamese Network Names/Datasets/'
    # long_file_path = 'drive/MyDrive/Siamese Network Names/Datasets/Phonetic_Encoding_Suggestions/'
    short_file_path = './Datasets/'
    long_file_path = './Datasets/Phonetic_Encoding_Suggestions/'

    # Parameter combinations for the network train ('configs')
    train_batch_size = [64]  # [52, 58, 64, 70, 76, 82]
    competitor_dataset = [True]  # [True,False] # true -> 2 versions , false -> new datasets only
    epochs = [10]  # [50,60,70,80,90,100,120]
    vector_split_type = ['word2top_grams']#['nam2vec_fasttext'] # ['word_ngrams']  # ,'name2vec']#['Sparse','Dense']
    name_letter_split = [2]  # [1,2,3] bgrams
    result_decision_distance = [6]  # [3, 4, 5, 6, 7, 8]
    datasets = ["knn_suggestions_according_sound_pandas_imp_sorted_by_ed.csv"]
    # dim parameters
    hidden_dim_one = [512]
    hidden_dim_two = [128]
    out_dim = [10]
    top_grams_amount = [10,50,100]

    gram_frequencies = {}
    # config = [epochs, vector_split_type, name_letter_split, result_decision_distance,
    #           datasets, train_batch_size, competitor_dataset]
    config = [epochs, vector_split_type, name_letter_split, result_decision_distance,
              datasets, train_batch_size, competitor_dataset,hidden_dim_one, hidden_dim_two,out_dim,top_grams_amount]
    for config_tuple in list(itertools.product(*config)):
        # activate all functions
        # dev = activate_GPU()
        # init config_tuple values
        epochs = config_tuple[0]
        vector_split_type = config_tuple[1]
        name_letter_split = config_tuple[2]
        result_decision_distance = config_tuple[3]
        datasets = config_tuple[4]
        train_batch_size = config_tuple[5]
        competitor_dataset = config_tuple[6]
        # dims parameters for the network architecture
        # TODO: change back after top grams trial
        # hidden_dim_one = config_tuple[7]
        # hidden_dim_two = config_tuple[8]
        # out_dim = config_tuple[9]
        dims = [hidden_dim_one,hidden_dim_two,out_dim]
        top_grams_amount = config_tuple[10]
        hidden_dim_one = int(top_grams_amount * 0.8)
        hidden_dim_two = int(top_grams_amount * 0.5)
        out_dim = int(top_grams_amount * 0.2)

        filtered_df = filter_names(short_file_path, datasets)  # filter names and save df to file

        # first phase train examples
        methods_df = phase_one_train.first_phase_negative_examples(long_file_path, competitor_dataset)
        train_df = phase_one_train.create_train_df(methods_df, filtered_df)
        train_data, test_data = phase_one_train.train_test_split(train_df)
        train_data = phase_one_train.add_random_negatives(train_df, train_data)  # add random easy negatives
        print("done phase1")
        # second phase train examples
        unique_df = phase_two_train.second_phase_negative_examples(short_file_path, datasets, filtered_df)
        trios = phase_two_train.add_random_negatives_phase_two(unique_df)
        train_data, test_data = phase_two_train.train_test_split_phase_two(train_data, test_data, trios)
        print("done phase2")

        # create and train the net
        train_data_vectors, embedded_dim = create_data_vectors(train_data, vector_split_type, name_letter_split,
                                                               gram_frequencies, top_grams_amount)
        train_dataloader = create_siamese_dataset(train_data_vectors, train_batch_size)
        net, last_epochs_avg = train_network(train_dataloader, epochs, embedded_dim, dims)
        print("done_train")
        df_train = prepare_data(train_data)
        # df_train -- anch, pos, label | anch, neg, label
        train_results, train_size = calculate_distances(train_data_vectors, train_batch_size, net, df_train)
        # df_train -- anch, pos, label, dist | anch, neg, label, dist
        df_train, train_accuracy, train_precision, train_recall, train_majority_same = calculate_accuracy(df_train,
                                                                                                          train_results,
                                                                                                          result_decision_distance)
        # df_train -- anch, pos, label, dist, actual, net_pred | anch, neg, label, dist, actual, net_pred
        train_scores = [train_size, train_accuracy, train_precision, train_recall, train_majority_same,
                        1 - train_majority_same]
        df_test = prepare_data(test_data)
        test_data_vectors, embedded_dim = create_data_vectors(test_data, vector_split_type, name_letter_split,
                                                              gram_frequencies , top_grams_amount)
        # save_gram_frequencies_dict(gram_frequencies)
        test_results, test_size = calculate_distances(test_data_vectors, train_batch_size, net, df_test)
        print("done_test")
        df_test, test_accuracy, test_precision, test_recall, test_majority_same = calculate_accuracy(df_test,
                                                                                                     test_results,
                                                                                                     result_decision_distance)
        test_scores = [test_size, test_accuracy, test_precision, test_recall, test_majority_same,
                       1 - test_majority_same]
        show_false_data_results(df_test)
        save_model_and_results(df_test, net, config_tuple)
        time_took = time.time() - start_time
        add_config_to_csv(config_tuple, train_scores, test_scores, last_epochs_avg, time_took)


def save_gram_frequencies_dict(gram_frequencies):
    with open('gram_frequencies_dictionary_new.pkl', 'wb') as f:
        pickle.dump(gram_frequencies, f)


def filter_rows_by_values(df, col, values):
    return df[~df[col].isin(values)]

def save_all(names):
    for name in names:
        try:
          with open("./fasttext_vecs/"+name+".pkl", 'rb') as f:
              word_vec = pickle.load(f)
        except:
          print("fell")
          with open('missing_names_old2.csv', 'a') as f_object:
            writer_object = writer(f_object, delimiter =' ')
            writer_object.writerow(name)
            f_object.close()

def filter_names(short_file_path, dataset):
    """
      The functions filter the names to remove and returns the new df
      The function saves the filtered df as a csv
    """
    vals_to_remove = ['Schleswigholsteinsonderburgglcksburg', 'Mariemargueriteyvetterollande',
                      'Schleswigholsteinsonderburg', 'Mariemadeleineconstance',
                      'Mariemargueriteanglique', 'Josephphilippenapoleon', 'Mariefranoiseangelique',
                      'Mariemadeleineanglique', 'Mariemargueritepascale', 'Jeanbaptistebarthlemy',
                      'Mariemadeleinemalvina', 'Mariemargueritejeanne', 'Charlesambroiseemery',
                      'Jeanbaptistevalentin', 'Jeanbaptistefrancois', 'Jeanbaptistenapoleon',
                      'Josephfrancoisarthur', 'Josephphilippehorace', 'Mariecatherinejoseph',
                      'Mariemargueriteagnes', 'Mariemargueriteccile', 'Jeanbaptisteadolphe',
                      'Jeanbaptisteauguste', 'Jeanbaptistemathieu', 'Jeanbaptisteantoine',
                      'Jeanbaptistefranois', 'Jeanbaptistebastien', 'Jeanbaptistegervais', 'Xxxxxxxxx']
    # The ground truth is pair of Original and Candidate with distance 0.
    # Pairs with distance 0 are pairs that sounded the same
    pd_reader = pd.read_csv(short_file_path + dataset)  # "knn_suggestions_according_sound_pandas_imp_sorted_by_ed.csv")
    filtered_df = pd_reader[pd_reader["Distance"] == 0]
    filtered_df = filter_rows_by_values(filtered_df, "Candidate", vals_to_remove)
    filtered_df = filter_rows_by_values(filtered_df, "Original", vals_to_remove)
    # save 'our' df as ground truth of spokenName2Vec
    filtered_df.to_csv("spokenName2Vec_ground_truth.csv")
    return filtered_df


# TODO: CHECK IF NEEDED
# Randomly choosing a negative sample (easy negative examples)
def negative(originals, index):
    neg = "None"
    # Each name might be in the dataset 10 times in a row (at most)
    if index <= 9:
        neg = originals[random.randint(10, len(originals) - 1)]
    elif index >= len(originals) - 10:
        neg = originals[random.randint(0, len(originals) - 10)]
    else:
        above = originals[random.randint(0, index - 10)]
        below = originals[random.randint(index + 10, len(originals) - 1)]
        neg = random.choice([above, below])
    return neg


"""
First method to create the train data
"""

"""
Second method to create the train-data.
"""


def create_data_vectors(data, split_method, name_letter_split, gram_frequencies_dict, top_grams_amount = 0):
    data_vectors = []
    dim = 28 ** 2

    if split_method == 'Sparse':
        for triplet in data:
            vec, dim = name_representation.word2sparse(triplet[0], name_letter_split, gram_frequencies_dict)
            data_vectors += [[vec.to(dev),
                              # data_vectors += [[word2sparse(triplet[0], name_letter_split, gram_frequencies_dict)[0].to(dev),
                              name_representation.word2sparse(triplet[1], name_letter_split, gram_frequencies_dict)[
                                  0].to(dev),
                              name_representation.word2sparse(triplet[2], name_letter_split, gram_frequencies_dict)[
                                  0].to(dev)]]
    # elif split_method == 'name2vec':
    #     for triplet in data:
    #         data_vectors += [[name2vec(triplet[0], name_letter_split).to(dev),
    #                           name2vec(triplet[1], name_letter_split).to(dev),
    #                           name2vec(triplet[2], name_letter_split).to(dev)]]
    elif split_method == 'nam2vec_fasttext': #old version
        for triplet in data:
            org = name_representation.nam2vec_fasttext(triplet[0], name_letter_split)[0]
            dim = name_representation.nam2vec_fasttext(triplet[0], name_letter_split)[1]
            if dim>0:
                data_vectors += [[org,
                                  name_representation.nam2vec_fasttext(triplet[1], name_letter_split)[0],
                                  name_representation.nam2vec_fasttext(triplet[2], name_letter_split)[0]]]
    elif split_method == 'word2top_grams':
        for triplet in data:
            org, dim = name_representation.word2top_grams(triplet[0], name_letter_split, top_grams_amount)
            data_vectors += [[org,
                              name_representation.word2top_grams(triplet[1], name_letter_split, top_grams_amount)[0],
                              name_representation.word2top_grams(triplet[2], name_letter_split,top_grams_amount)[0]]]

    elif split_method == 'word_ngrams':
        for triplet in data:
            org = name_representation.word_ngrams(triplet[0], name_letter_split)[0]
            dim = name_representation.word_ngrams(triplet[0], name_letter_split)[1]
            data_vectors += [[org,
                              name_representation.word_ngrams(triplet[1], name_letter_split)[0],
                              name_representation.word_ngrams(triplet[2], name_letter_split)[0]]]
    else:
        for triplet in data:
            data_vectors += [[name_representation.word2dense(triplet[0], name_letter_split)[0].to(dev),
                              name_representation.word2dense(triplet[1], name_letter_split)[0].to(dev),
                              name_representation.word2dense(triplet[2], name_letter_split)[0].to(dev)]]
    return data_vectors, dim


"""## **Siamese Network**
**With PyTorch**

building and training the network
"""


def create_siamese_dataset(train_data_vectors, train_batch_size):
    """
    The method creates the siamese dataset and wraps with dataloader to loop through the set rows in batches
    """
    siamese_dataset = siamese_network.SiameseNetworkDataset(train_data_vectors)
    # DataLoaders help arrange the data
    train_dataloader = DataLoader(siamese_dataset,
                                  shuffle=True,
                                  num_workers=0,
                                  batch_size=train_batch_size)
    return train_dataloader


"""You can read about *TripletMarginLoss* [here](https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html)."""


def show_plot(iteration, loss):
    plt.plot(iteration, loss)
    plt.show()


# def train_network(train_dataloader, train_number_epochs, embedded_dim):
def train_network(train_dataloader, train_number_epochs, embedded_dim, hidden_dims ): #dims_list
    """
    Train the network and calculate loss
    """
    # net = siamese_network.SiameseNetwork(embedded_dim).to(dev)  # embedded_dim = 28 ** 2
    net = siamese_network.SiameseNetwork(embedded_dim,hidden_dims[0],hidden_dims[1], hidden_dims[2]).to(dev)  # embedded_dim = 28 ** 2
    # if you get "expected double" error while trying to train the model, add:
    # net = net.double()
    criterion = nn.TripletMarginLoss(margin=0.5)
    optimizer = optim.Adam(net.parameters(), lr=0.0005)
    counter = []
    loss_history = []
    iteration_number = 0

    net.train()
    last_epochs_avg = 0
    for epoch in range(0, train_number_epochs):
        T = time.time()
        for i, data in enumerate(train_dataloader, 0):
            anchors, positives, negatives = data
            negatives = torch.as_tensor(negatives).to(dev)
            anchors, positives = torch.as_tensor(anchors).to(dev), torch.as_tensor(positives).to(dev)
            optimizer.zero_grad()
            output1, output2, output3 = net(anchors, positives, negatives) # forward
            loss_contrastive = criterion(output1, output2, output3)
            loss_contrastive.backward()
            optimizer.step()

            # print starting value
            if epoch == 0 and i == 0:
                print("Start\n Current loss {}\n".format(loss_contrastive.item()))
                iteration_number += 10
                counter.append(iteration_number)
                loss_history.append(loss_contrastive.item())

        print("Epoch number {}\n Current loss {}\n".format(epoch, loss_contrastive.item()))
        ### added code for avg epoch loss
        if epoch + 3 >= train_number_epochs:
            last_epochs_avg += loss_contrastive.item()
        ##################3
        iteration_number += 10
        counter.append(iteration_number)
        loss_history.append(loss_contrastive.item())
        T = time.time() - T
        print("Time: {}\n".format(T))
    #####
    last_epochs_avg = last_epochs_avg / 3
    print("last 3 epochs average: ", last_epochs_avg)
    #####
    net.eval()
    show_plot(counter, loss_history)
    return net, last_epochs_avg


def prepare_data(data):
    """
    The function creates an anchor, candidate, label row for each row item in the data given
    """
    # Creating DataFrame from the test-data
    anch = []
    candidate = []
    labels = []
    # create 2 rows out of each tuple row in the data -->
    # anchor, candidate_pos, positive | anchor, candidate_neg, negative
    for row in data:
        # row1
        anch += [row[0]]
        candidate += [row[1]]
        labels += ['Positive']
        # row2
        anch += [row[0]]
        labels += ['Negative']
        candidate += [row[2]]
    data = {'Original': anch, 'Candidate': candidate, 'Type': labels}
    df = pd.DataFrame(data)
    return df


def calculate_distances(data_vectors, batch_size, net, df):
    siamese_dataset = siamese_network.SiameseNetworkDataset(data_vectors)

    results = []
    dataloader = DataLoader(siamese_dataset, num_workers=0, batch_size=batch_size, shuffle=False)
    data_iter = iter(dataloader)
    dist = []
    for i in range(len(dataloader)):
        x0, x1, label2 = next(data_iter)
        x0, x1, label2 = torch.as_tensor(x0).to(dev), torch.as_tensor(x1).to(dev), torch.as_tensor(label2).to(dev)
        output1, output2, output3 = net(x0, x1, label2)

        # for each trio we calculate:
        # 1. The distance of the positive sample from the original sample
        # 2. The distance of the negative sample from the original sample
        for j in range(len(output1)):
            euclidean_distance1 = F.pairwise_distance(output1[j], output2[j])
            euclidean_distance2 = F.pairwise_distance(output1[j], output3[j])
            results += [[euclidean_distance1.item(), euclidean_distance2.item()]]
            # for idx in range(len(euclidean_distance1)):
            dist += [euclidean_distance1.item()]
            dist += [euclidean_distance2.item()]
    # dist
    df['Distance'] = dist

    return results, len(dist)


def calculate_accuracy(df, results, result_decision_distance):
    # We use the distances to predict whether two names sound the same (experiment with the distance.. maybe mean)
    actual = []  # the real type of the pair
    net_result = []  # the result from the network
    for r in results:
        if r[0] <= result_decision_distance:
            actual += ['same']
            net_result += ['same']
        else:
            actual += ['same']
            net_result += ['different']
        if r[1] <= result_decision_distance:
            net_result += ['same']
            actual += ['different']
        else:
            net_result += ['different']
            actual += ['different']
    df['Actual'] = actual
    df['Network_Prediction_Result'] = net_result
    accuracy = accuracy_score(actual, net_result)
    print("Accuracy ", accuracy)
    ##PRECISION
    macro = precision_score(net_result, actual, average='macro')
    micro = precision_score(net_result, actual, average='micro')
    weighted = precision_score(net_result, actual, average='weighted')
    print("Precision: macro = " + str(macro), "micro = " + str(micro), "weighted = " + str(weighted))
    precision = weighted
    ##RECALL
    macro = recall_score(net_result, actual, average='macro')
    micro = recall_score(net_result, actual, average='micro')
    weighted = recall_score(net_result, actual, average='weighted')
    print("Recall: macro = " + str(macro), "micro = " + str(micro), "weighted = " + str(weighted))
    recall = weighted
    # check the accuracy - what is the majority and if they are always chosen
    majority_same = df['Actual'].value_counts()['same'] / df['Actual'].size
    return df, accuracy, precision, recall, majority_same


def show_false_data_results(df_test):
    """
    The method prints useful data results of false predictions by the network
    """
    print('FalseDataResults:')
    # print(df_test)
    print(df_test[df_test["Actual"] != df_test["Network_Prediction_Result"]])
    print("Actual: same, Prediction: different ",
          df_test[((df_test["Actual"] == 'same') & (df_test["Network_Prediction_Result"] == 'different'))][
              "Distance"].mean())
    print("Actual: different, Prediction: same ",
          df_test[((df_test["Actual"] == 'different') & (df_test["Network_Prediction_Result"] == 'same'))][
              "Distance"].mean())
    # print(df_test[((df_test["Actual"] == 'same') & (df_test["Network_Prediction_Result"]=='different'))])


"""xfff"""


def save_model_and_results(df_test, net, config_tuple):  # net? to save
    # Saving the results to csv allows us to look on all the samples
    df_test.to_csv('./TestDataResults/Test_Data_and_Results_' + str(config_tuple) + '.csv')
    # Saving the model
    # PATH = 'model.pkl'
    # torch.save(net.state_dict(), PATH)


# append the tuples to the csv file
def add_config_to_csv(config_tuple, train_scores, test_scores, last_epochs_avg, time_took):
    tup = []
    # add configuration values to the parameters file
    for config in config_tuple:
        tup += [config]

    tup += train_scores + test_scores + [last_epochs_avg] + [time_took]
    # tup += [accuracy] + [precision] + [recall] + [majority_same] + [1 - majority_same]  # diff
    print(tup)
    # fields = ['Epochs','Vector_Split_Type','Name_Letter_Split','Result_Decision_Distance',
    #           'Datasets','Train_Batch_Size']
    # Open our existing CSV file in append mode
    # Create a file object for this file
    with open('./Siamese_Parameter_Runs.csv', 'a') as f_object:
        writer_object = writer(f_object)
        writer_object.writerow(tup)
        # Close the file object
        f_object.close()


main_activation()

"""**Precision**

**Recall**

# 
# ###**Small Test**
# Using small dataset in order to save time for a simple test
# """
#
# test_df_first = pd.read_csv("all_distinct_names_length_higher_than_2_characters.csv") # a list of first names
# test_data = test_df_first['Name']
# tmp = []
# for i in range(0, len(test_data), 3):
#   if i + 2 < len(test_data):
#     tmp += [[test_data[i], test_data[i + 1], test_data[i + 2]]]
# test_data = tmp
# names = tmp
#
# vectors_b = []
# for triplet in test_data:
#   vectors_b += [[word2sparse(triplet[0]).to(dev),
#                  word2sparse(triplet[1]).to(dev),
#                  word2sparse(triplet[2]).to(dev)]]
# test_data_vectors = vectors_b
#
# siamese_dataset = SiameseNetworkDataset(test_data_vectors)
# test_dataloader = DataLoader(siamese_dataset, num_workers=0, batch_size=64, shuffle=False)
# dataiter = iter(test_dataloader)
#
# results = []
# dim = 10
#
# for x0,x1, x2 in dataiter:
#     output1,output2, output3 = net(torch.as_tensor(x0).to(dev),torch.as_tensor(x1).to(dev), torch.as_tensor(x2).to(dev))
#     results += [[output1, output2, output3]]
#
# count = 0
# names_list =[]
# outputs_list = []
# for r in results:
#   for j in range(len(r[0])):
#     names_list += [names[count][0], names[count][1], names[count][2]]
#     outputs_list += [r[0][j], r[1][j],  r[2][j]]
#     count += 1
# emmbedings = []
# for i in range(dim):
#   emmbedings += [[]]
# print(len(emmbedings))
# for output in tqdm(outputs_list):
#   for j in range(dim):
#     emmbedings[j] += [output[0][j].item()]
#
# list_for_dict = [('Name', names_list)]
# for i, e in enumerate(emmbedings):
#   list_for_dict += [('e{}'.format(i), emmbedings[i])]
#
# data = dict(list_for_dict)
# emm_df = pd.DataFrame(data)
# data
#
# """Skip 7 blocks to continue this test
#
# # **Test**
# """
#
# test_df_first = pd.read_csv("all_distinct_First_Name_wikitree.csv") # list of first names and synonyms
# test_data = test_df_first['Child_First_Name']
# tmp = []
# for i in range(0, len(test_data), 3):
#   if i + 2 < len(test_data):
#     tmp += [[test_data[i], test_data[i + 1], test_data[i + 2]]]
# test_data = tmp
# names = tmp
#
# def transform_data(data):
#   transformed_data = []
#   for threesome in test_data:
#     transformed_data += [[word2sparse(threesome[0]).to(dev),
#                           word2sparse(threesome[1]).to(dev),
#                           word2sparse(threesome[2]).to(dev)]]
#   return transformed_data
#
# # DataLoader sometumes has problems with big lists
# data1 = test_data[:50000]
# data2 = test_data[50000:100000]
# data3 = test_data[100000:150000]
# data4 = test_data[150000:200000]
# data5 = test_data[200000:]
# data1 = transform_data(data1)
# data2 = transform_data(data2)
# data3 = transform_data(data3)
# data4 = transform_data(data4)
# data5 = transform_data(data5)
#
# data1_part1 = data1[:25000]
# data2_part1 = data2[:25000]
# data3_part1 = data3[:25000]
# data4_part1 = data4[:25000]
# data5_part1 = data5[:25000]
# data1_part2 = data1[25000:]
# data2_part2 = data2[25000:]
# data3_part2 = data3[25000:]
# data4_part2 = data4[25000:]
# data5_part2 = data5[25000:]
#
# data = [data1_part1, data1_part2, data2_part1, data2_part2, data3_part1,
#         data3_part2, data4_part1, data4_part2, data5_part1, data5_part2]
#
# for subdata in data:
#   siamese_dataset = SiameseNetworkDataset(subdata)
#   test_dataloader = DataLoader(siamese_dataset, num_workers=2, batch_size=64, shuffle=False, collate_fn=collate)
#   for x0,x1,x2 in tqdm(test_dataloader):
#     # x0,x1,x2 = data
#     x2 = x2.cuda()
#     output1,output2, output3 = net(torch.as_tensor(x0).cuda(),torch.as_tensor(x1).cuda(), x2)
#     results += [[output1, output2, output3]]
#
# count = 0
# names_list =[]
# outputs_list = []
# for r in results:
#   for j in range(len(r[0])):
#     try:
#       names_list += [names[count][0], names[count][1], names[count][2]]
#       outputs_list += [r[0][j], r[1][j],  r[2][j]]
#       count += 1
#     except:
#       print("index:", count)
# emmbedings = [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]
# print(len(emmbedings))
# for output in tqdm(outputs_list):
#   emmbedings[0] += [output[0].item()]
#   emmbedings[1] += [output[1].item()]
#   emmbedings[2] += [output[2].item()]
#   emmbedings[3] += [output[3].item()]
#   emmbedings[4] += [output[4].item()]
#   emmbedings[5] += [output[5].item()]
#   emmbedings[6] += [output[6].item()]
#   emmbedings[7] += [output[7].item()]
#   emmbedings[8] += [output[8].item()]
#   emmbedings[9] += [output[9].item()]
#   # emmbedings[10] += [output[10].item()]
#   # emmbedings[11] += [output[11].item()]
#   # emmbedings[12] += [output[12].item()]
#   # emmbedings[13] += [output[13].item()]
#   # emmbedings[14] += [output[14].item()]
#   # emmbedings[15] += [output[15].item()]
#   # emmbedings[16] += [output[16].item()]
#   # emmbedings[17] += [output[17].item()]
#   # emmbedings[18] += [output[18].item()]
#   # emmbedings[19] += [output[19].item()]
#   # emmbedings[20] += [output[20].item()]
#   # emmbedings[21] += [output[21].item()]
#   # emmbedings[22] += [output[22].item()]
#   # emmbedings[23] += [output[23].item()]
#   # emmbedings[24] += [output[24].item()]
#   # emmbedings[25] += [output[25].item()]
#
# data = {'Name': names_list, 'e0': emmbedings[0], 'e1': emmbedings[1],
#         'e2': emmbedings[2], 'e3': emmbedings[3], 'e4': emmbedings[4],
#         'e5': emmbedings[5], 'e6': emmbedings[6], 'e7': emmbedings[7],
#         'e8': emmbedings[8], 'e9': emmbedings[9]} # , 'e10': emmbedings[10],
#         # 'e11': emmbedings[11], 'e12': emmbedings[12], 'e13': emmbedings[13],
#         # 'e14': emmbedings[14], 'e15': emmbedings[15], 'e16': emmbedings[16],
#         # 'e17': emmbedings[17], 'e18': emmbedings[18], 'e19': emmbedings[19],
#         # 'e20': emmbedings[20], 'e21': emmbedings[21], 'e22': emmbedings[22],
#         # 'e23': emmbedings[23], 'e24': emmbedings[24], 'e25': emmbedings[25]}
# emm_df = pd.DataFrame(data)
#
# emm_df
#
# feature_names = []
# for i in range(dim):
#     feature_name = f"e{i}"
#     feature_names.append(feature_name)
#
# data = tc.SFrame(emm_df)
# data.shape
#
# """# We use the results from the siamese network and knn in order to find 10 nearest names."""
#
# model = tc.nearest_neighbors.create(data, features=feature_names)
#
# # calculate KNN for all names
# knn = model.query(data, k=11)
# # remove the name itself from its list
# sf = knn[knn['query_label'] != knn['reference_label']]
# sf.materialize()
#
# sf.export_csv('knn_results_with_indexes.csv')
# print("Done!!")
#
# suggestions_df = sf.to_dataframe()
#
# test_df_first = test_df_first.reset_index()
#
# names_dict = {"name": names_list + ["unk"], "index": test_df_first.index}
# # You should check why are the lists don't have the same length
# names_df = pd.DataFrame(names_dict)
#
# names_df
#
# knn_suggestions_df = pd.merge(suggestions_df, names_df, how="inner", left_on='query_label', right_on='index')
#
# knn_suggestions_df = knn_suggestions_df.rename(columns={"index": "Original_index",
#                                                         "name": "Original"})
# knn_suggestions_df
#
# knn_suggestions_df = pd.merge(knn_suggestions_df, names_df, how="inner", left_on='reference_label', right_on='index')
# knn_suggestions_df = knn_suggestions_df.rename(columns={"index": "Candidate_index",
#                                                         "name": "Candidate",
#                                                        "distance": "Distance",
#                                                        "rank": "Rank"})
# knn_suggestions_df = knn_suggestions_df[["Original", "Candidate", "Distance", "Rank"]]
# knn_suggestions_df = knn_suggestions_df.sort_values(by=['Original', 'Rank'], ascending=True)
# knn_suggestions_df
#
# knn_suggestions_df.to_csv("Final_Results.csv")
#
# knn_suggestions_better_df = knn_suggestions_df[knn_suggestions_df['Distance'] == 0]
# knn_suggestions_better_df.to_csv("Final_Results_Zeros.csv")
#
# knn_suggestions_better_df
